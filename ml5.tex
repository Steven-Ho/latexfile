\documentclass[UTF8]{ctexart}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{latexsym}
\usepackage{indentfirst}
\usepackage{amssymb}
\usepackage{tikz}
%\usepackage[top=1in,bottom=1in,left=0.5in,right=0.5in]{geometry}
\usepackage{ulem}
\setlength{\parindent}{2em}
\usepackage{color}
\definecolor{lbcolor}{rgb}{0.9,0.9,0.9}
\usepackage{listings}
\lstset{backgroundcolor=\color{lbcolor}}
\lstset{keywordstyle=\color[rgb]{0,0,1}}
\lstset{commentstyle=\color[rgb]{0.133,0.545,0.133}}
\lstset{stringstyle=\color[rgb]{0.627,0.126,0.941}}
\lstset{language=Matlab}
\lstset{numbers=left}
\lstset{breaklines=true}
\usepackage{amsmath}
\author{何舜成}
\date{2012011515}
\title{Machine Learning Exercise 5}
\begin{document}
\maketitle
\textbf{1. Show KKT condition is necessary condition and sufficient in some cases}:\par
\uline{Proof}:\par
A standard form of optimal problem can be described as follow\par
\begin{eqnarray*}
 &min&f_{0}(x)\\
&s.t.&f_{i}(x)\leq 0,i=1,\cdots,m\\
& &h_{i}(x)=0,i=1,\cdots,p
\end{eqnarray*}\par
where $x\in\mathcal{R}^{n}$ and the domian $\mathcal{D}=\bigcap_{i=0}^{m}\textbf{dom}f_{i}\cap\bigcap_{i=1}^{p}\textbf{dom}h_{i}$ not empty. And the optimal solution is $p^{*}$.\par
Define Lagrangian function\par
\begin{equation*}
L(x,\lambda,\mu)=f_{0}(x)+\sum_{i=1}^{m}\lambda_{i}f_{i}(x)+\sum_{i=1}^{p}\mu_{i}h_{i}(x)
\end{equation*}\par
where $\lambda$ and $\mu$ are called Lagrangian multiplier.\par
Define Lagrangian dual function\par
\begin{equation*}
g(\lambda,\mu)=\inf_{x\in\mathcal{D}}L(x,\lambda,\mu)=\inf_{x\in\mathcal{D}}(f_{0}(x)+\sum_{i=1}^{m}\lambda_{i}f_{i}(x)+\sum_{i=1}^{p}\mu_{i}h_{i}(x))
\end{equation*}\par
Suppose $\tilde{x}$ is feasible to primal problem, then for all $\lambda\geq 0$ and $\mu$\par
\begin{equation*}
\sum_{i=1}^{m}\lambda_{i}f_{i}(x)+\sum_{i=1}^{p}\mu_{i}h_{i}(x)\leq 0
\end{equation*}\par
\begin{equation*}
L(\tilde{x},\lambda,\mu)=f_{0}(\tilde{x})+\sum_{i=1}^{m}\lambda_{i}f_{i}(x)+\sum_{i=1}^{p}\mu_{i}h_{i}(x)\leq f_{0}(\tilde{x})
\end{equation*}\par
therefore\par
\begin{equation*}
g(\lambda,\mu)=\inf_{x\in\mathcal{D}}L(x,\lambda,\mu)\leq L(\tilde{x},\lambda,\mu)\leq f_{0}(\tilde{x})
\end{equation*}\par
Consider the Lagrangian dual problem\par
\begin{eqnarray*}
 &max&g(\lambda,\mu)\\
&s.t.&\lambda_{i}\geq 0,i=1,\cdots,m
\end{eqnarray*}\par
the dual problem is convex optimal problem. If $(\lambda^{*},\mu^{*})$ is the optimal solution and the target function reaches $d^{*}$ at $(\lambda^{*},\mu^{*})$, we easily get this inequality\par
\begin{equation*}
d^{*}\leq p^{*}
\end{equation*}\par
and we call this weak duality. Likewise, if\par
\begin{equation*}
d^{*}=p^{*}
\end{equation*}\par
holds true, we call this strong dualtiy.\par
In condition of strong duality and the existence of optimal solution $x^{*}$ of primal problem, we have\par
\begin{eqnarray}
f_{0}(x^{*})&=&g(\lambda^{*},\mu^{*})\\
&=&\inf_{x}(f_{0}(x)+\sum_{i=1}^{m}\lambda_{i}^{*}f_{i}(x)+\sum_{i=1}^{p}\mu_{i}^{*}h_{i}(x))\\
&\leq&f_{0}(x^{*})+\sum_{i=1}^{m}\lambda_{i}^{*}f_{i}(x^{*})+\sum_{i=1}^{p}\mu_{i}^{*}h_{i}(x^{*})\\
&\leq&f_{0}(x^{*})
\end{eqnarray}\par
Therefore the inequalities are forced to be equal. We can infer\par
\begin{equation*}
\left.\frac{\partial L(x,\lambda^{*},\mu^{*})}{\partial x}\right|_{x=x^{*}}=0
\end{equation*}\par
from the third equation (supposing $f_{i}$ and $h_{i}$ are differentiable), and infer\par
\begin{equation*}
\sum_{i=1}^{m}\lambda_{i}^{*}f_{i}(x^{*})=0
\end{equation*}\par
or $\lambda_{i}^{*}f_{i}(x^{*})=0,i=1,\cdots,m$ from the fourth equation.\par
To conclude, if $x^{*}$ and $(\lambda^{*},\mu^{*})$ are optimal solutions of the primal and dual problem respectively, and strong duality is sufficed, KKT condition\par
(1) $x^{*}$ is primal feasible\par
\begin{equation*}
f_{i}(x)\leq 0,i=1,\cdots,m
\end{equation*}\par
\begin{equation*}
h_{i}(x)=0,i=1,\cdots,p
\end{equation*}\par
(2) $(\lambda^{*},\mu^{*})$ are dual feasible\par
\begin{equation*}
\lambda^{*}\geq 0,i=1,\cdots,m
\end{equation*}\par
(3) complementary slackness\par
\begin{equation*}
\lambda^{*}f_{i}(x^{*})=0,i=1,\cdots,m
\end{equation*}\par
(4) stationary\par
\begin{equation*}
\nabla f_{0}(x^{*})+\sum_{i=1}^{m}\lambda_{i}^{*}\nabla f_{i}(x^{*})+\sum_{i=1}^{p}\mu_{i}^{*}\nabla h_{i}(x^{*})=0
\end{equation*}\par
will hold true simultaneously. KKT condition is \textbf{necessary}.\par
KKT condition is also sufficient if\par
\begin{equation*}
\left.\frac{\partial L(x,\lambda^{*},\mu^{*})}{\partial x}\right|_{x=x^{*}}=0\Rightarrow L(x^{*},\lambda^{*},\mu^{*})=\inf_{x\in\mathcal{D}}(x,\lambda^{*},\mu^{*})
\end{equation*}\par
\uline{Proof}:\par
Combining the condition above with the (4) equation in KKT condition, we get\par
\begin{equation*}
g(\lambda^{*},\mu^{*})=L(x^{*},\lambda^{*},\mu^{*})
\end{equation*}\par
According to other 3 conditions, we know\par
\begin{equation*}
g(\lambda^{*},\mu^{*})=f_{0}(x^{*})
\end{equation*}\par
Therefore we can infer $x^{*}$ and $(\lambda^{*},\mu^{*})$ are optimal solutions of primal and dual problem respectively from weak duality. KKT condition is \textbf{sufficient}.\par
\vspace{10pt}
\textbf{2. Give the dual problem of SVM when linear inseparable}\par
Slackness variables are introduces when data are linear inseparable. The primal problem\par
\begin{eqnarray*}
 &min&\frac{1}{2}||w||^{2}+C\sum_{i=1}^{n}\xi_{i}\\
&s.t.&y_{i}(w^{T}x_{i}+b)\geq 1-\xi_{i},\forall i\in[n]\\
& &\xi_{i}\geq 0,\forall i\in[n]
\end{eqnarray*}\par
The Lagrangian function is\par
\begin{equation*}
L(w,b,\xi,\alpha,r)=\frac{1}{2}||w||^{2}+C\sum_{i=1}^{n}\xi_{i}-\sum_{i=1}^{n}\alpha_{i}(y_{i}(w^{T}x_{i}+b)-1+\xi_{i})-\sum_{i=1}^{n}r_{i}\xi_{i}
\end{equation*}\par
for all $i\in[n],\alpha_{i}\geq 0,r_{i}\geq 0$.\par
Set the partial derivatives to zero, and we get\par
\begin{eqnarray*}
\frac{\partial L}{\partial w}=0&\Rightarrow&w=\sum_{i=1}^{n}\alpha_{i}y_{i}x_{i}\\
\frac{\partial L}{\partial b}=0&\Rightarrow&\sum_{i=1}^{n}\alpha_{i}y_{i}=0\\
\frac{\partial L}{\partial \xi_{i}}=0&\Rightarrow&C-\alpha_{i}-r_{i}=0,\forall i\in[n]
\end{eqnarray*}\par
According to the previous exercise,\par
\begin{equation*}
L(w,b,\xi,\alpha,r)=\sum_{i=1}^{n}\alpha_{i}-\frac{1}{2}\sum_{i,j=1}^{n}\alpha_{i}\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j}
\end{equation*}\par
Therefore the dual can be described as follow\par
\begin{eqnarray*}
 &max&\sum_{i=1}^{n}\alpha_{i}-\frac{1}{2}\sum_{i,j=1}^{n}\alpha_{i}\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j}\\
&s.t.&0\leq\alpha_{i}\leq C,\forall i\in[n]\\
& &\sum_{i=1}^{n}\alpha_{i}y_{i}=0
\end{eqnarray*}\par
\vspace{10pt}
\textbf{3. Design an algorithm to calculate gradient of the loss function of neural network}\par
Mathematical model of artificial neural network:\par
\begin{equation*}
x^{l}=f(u^{l}), u^{l}=(W^{l-1})^{T}x^{l-1}+b^{l}
\end{equation*}\par
where $l$ denotes the current layer with the output layer designated to be layer $L$ and the input layer desiganted to ba layer $1$. Function $f(\cdot)$ is a nonlinear function (i.e. sigmoid or hyperbolic tangent).Define the loss function as\par
\begin{equation*}
E(x^{L},t)
\end{equation*}\par
where $x^{L}$ is the network output and $t$ is the target output. Usually we choose\par
\begin{equation*}
E(x^{L},t)=\frac{1}{2}||t-x^{L}||^{2}
\end{equation*}\par
Since\par
\begin{equation*}
E(x^{L},t)=E(f((W^{L-1})^{T}x^{L-1}),t)
\end{equation*}
we can write the derivatives w.r.t. $W^{L-1}$\par
\begin{equation*}
\frac{\partial E}{\partial W^{L-1}}=x^{L-1}(f'(u^{L})\star\frac{\partial E}{\partial x^{L}})^{T}
\end{equation*}\par
where $\star$ denotes elementwise multipying, and if we define\par
\begin{equation*}
\delta^{L}=f'(u^{L})\star\frac{\partial E}{\partial x^{L}}
\end{equation*}\par
we get\par
\begin{equation*}
\frac{\partial E}{\partial W^{L-1}}=x^{L-1}(\delta^{L})^{T}
\end{equation*}\par
If we calculate the $\delta$ term recursively\par
\begin{equation*}
\delta^{l}=f'(u^{l})\star((W^{l})^{T}\delta^{l+1}),l=L-1,\cdots,2
\end{equation*}\par
it is easy to write\par
\begin{equation*}
\frac{\partial E}{\partial W^{l}}=x^{l}(\delta^{l+1})^{T},l=L-2,\cdots,1
\end{equation*}
\end{document}